# ai_summarizer.py
import logging
import sys
from typing import Optional
from config import GitReportConfig
import os

# --- (V3.4) å¯¼å…¥æŠ½è±¡å±‚å’Œå…·ä½“ç­–ç•¥ ---
from llm.provider_abc import LLMProvider
from llm.gemini_provider import GeminiProvider
from llm.deepseek_provider import DeepSeekProvider

# --- (V3.4) ç»“æŸ ---


try:
    import google.generativeai as genai
except ImportError:
    print("é”™è¯¯: google-generativeai åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install google-generativeai")
    pass

logger = logging.getLogger(__name__)


# --- (V3.4) å·¥å‚å‡½æ•° ---
def get_llm_provider(provider_id: str, config: GitReportConfig) -> LLMProvider:
    """
    (V3.4) å·¥å‚å‡½æ•°ï¼Œæ ¹æ® provider_id é€‰æ‹©å¹¶å®ä¾‹åŒ–æ­£ç¡®çš„ LLM ä¾›åº”å•†
    è¿™æ˜¯ç­–ç•¥æ¨¡å¼é€‰æ‹©çš„æ ¸å¿ƒã€‚
    """
    logger.info(f"â„¹ï¸ (V3.4) æ­£åœ¨å°è¯•åˆå§‹åŒ– LLM ä¾›åº”å•†: {provider_id}")

    # (V3.4) éªŒè¯æ‰€é€‰ä¾›åº”å•†æ˜¯å¦å·²è®¾ç½®å…¶å¯†é’¥
    if not config.is_provider_configured(provider_id):
        logger.error(f"âŒ (V3.4) ä¾›åº”å•† '{provider_id}' æœªé…ç½®ã€‚")
        raise ValueError(
            f"ä¾›åº”å•† '{provider_id}' æœªé…ç½®ã€‚ "
            f"è¯·åœ¨æ‚¨çš„ .env æ–‡ä»¶ä¸­è®¾ç½®ç›¸åº”çš„ API å¯†é’¥ã€‚"
        )

    # (V3.4) ç­–ç•¥é€‰æ‹©
    try:
        if provider_id == "gemini":
            return GeminiProvider(config)
        elif provider_id == "deepseek":
            return DeepSeekProvider(config)

        # (V3.4) æœªçŸ¥ä¾›åº”å•†çš„å›é€€
        logger.error(f"âŒ (V3.4) æœªçŸ¥çš„ LLM ä¾›åº”å•†: {provider_id}")
        raise ValueError(f"æœªçŸ¥çš„ LLM ä¾›åº”å•†: {provider_id}")
    except ImportError as e:
        logger.error(f"âŒ (V3.4) å¯¼å…¥ä¾›åº”å•† '{provider_id}' å¤±è´¥ã€‚")
        logger.error(
            f"   è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…éœ€çš„ä¾èµ– (ä¾‹å¦‚ 'pip install google-generativeai openai')ã€‚"
        )
        raise ImportError(f"ä¾›åº”å•† '{provider_id}' ä¾èµ–ç¼ºå¤±: {e}")
    except Exception as e:
        # æ•è· Gemini/DeepSeek __init__ ä¸­çš„å…¶ä»–å¼‚å¸¸
        logger.error(f"âŒ (V3.4) å®ä¾‹åŒ–ä¾›åº”å•† '{provider_id}' å¤±è´¥: {e}")
        raise


# (V2.4 é‡æ„: æ•´ä¸ªæ–‡ä»¶è¢«é‡æ„ä¸º AIService ç±»)
class AIService:
    """
    (V3.4 é‡æ„) å°è£…æ‰€æœ‰å¯¹ LLM çš„è°ƒç”¨ã€‚
    åœ¨åˆå§‹åŒ–æ—¶é…ç½®ä¸€æ¬¡ä¾›åº”å•† (ç­–ç•¥)ã€‚
    """

    def __init__(self, config: GitReportConfig, provider_id: str):
        """
        (V3.4 ä¿®æ”¹) åˆå§‹åŒ– AI æœåŠ¡
        - provider_id: [V3.4] ç”¨æˆ·é€‰æ‹©çš„ä¾›åº”å•† ID (ä¾‹å¦‚ "gemini") ã€‚
        """
        self.config = config

        # (V3.4) AIService æŒæœ‰ä¸€ä¸ªå¯¹ "Strategy" (LLMProvider) çš„å¼•ç”¨
        # å®ƒä»å·¥å‚è·å–è¿™ä¸ªä¾›åº”å•† ã€‚
        # å¦‚æœ provider_id æ— æ•ˆæˆ–æœªé…ç½®ï¼Œå·¥å‚å°†å¼•å‘ ValueErrorã€‚
        self.provider: LLMProvider = get_llm_provider(provider_id, config)

        logger.info(
            f"âœ… ğŸ¤– AI æœåŠ¡å·²æˆåŠŸåˆå§‹åŒ– (Provider: {self.provider.__class__.__name__})"
        )

    # (V3.4) ç§»é™¤: _configure_genai(self)
    # æ­¤é€»è¾‘ç°å·²ç§»è‡³ llm/gemini_provider.py

    # --- (V3.3) Prompt åŠ è½½å™¨ (V3.4 ä¿æŒä¸å˜) ---
    def _load_prompt_template(self, template_name: str) -> Optional[str]:
        """(V3.3) è¾…åŠ©å‡½æ•°ï¼šä» prompts/ ç›®å½•åŠ è½½æ¨¡æ¿"""
        prompt_path = os.path.join(
            self.config.SCRIPT_BASE_PATH, "prompts", template_name
        )
        try:
            with open(prompt_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            logger.error(f"âŒ (V3.3) Prompt æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {prompt_path}")
            return None
        except Exception as e:
            logger.error(f"âŒ (V3.3) åŠ è½½ Prompt æ¨¡æ¿å¤±è´¥ ({prompt_path}): {e}")
            return None

    # --- (V3.4) é‡æ„æ‰€æœ‰ AI è°ƒç”¨æ–¹æ³• ---

    def _generate_content(
        self, prompt_template_name: str, format_kwargs: dict
    ) -> Optional[str]:
        """
        (V3.4 æ–°å¢) å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼Œç”¨äºç»Ÿä¸€è°ƒç”¨ self.providerã€‚
        """
        if not self.provider:
            logger.error("âŒ (V3.4) AI Provider æœªåˆå§‹åŒ–ã€‚")
            return None

        # (V3.3) åŠ è½½ Prompt
        prompt_template = self._load_prompt_template(prompt_template_name)
        if not prompt_template:
            return None

        # (V3.3) æ ¼å¼åŒ– Prompt
        try:
            user_prompt = prompt_template.format(**format_kwargs)
        except KeyError as e:
            logger.error(
                f"âŒ (V3.4) æ ¼å¼åŒ– Prompt '{prompt_template_name}' å¤±è´¥: ç¼ºå°‘é”® {e}"
            )
            return None

        logger.info(f"ğŸ¤– æ­£åœ¨è°ƒç”¨ AI Provider: {self.provider.__class__.__name__}...")

        try:
            # (V3.4) å°†å·¥ä½œå§”æ‰˜ç»™é€‰å®šçš„ä¾›åº”å•†
            # æˆ‘ä»¬å°† V3.3 å®Œæ•´çš„ã€å·²æ ¼å¼åŒ–çš„æç¤ºä½œä¸º 'user_prompt' ä¼ é€’ã€‚
            # 'system_prompt' æš‚æ—¶ä¸ºç©ºã€‚
            # V3.5 å°†é€šè¿‡ä¿®æ”¹æ­¤å¤„çš„é€»è¾‘æ¥å®ç°æç¤ºè¯å·®å¼‚åŒ– ã€‚
            system_prompt = ""

            response_text = self.provider.generate_summary(
                system_prompt=system_prompt, user_prompt=user_prompt
            )

            logger.info(f"âœ… AI Provider è°ƒç”¨æˆåŠŸ ({prompt_template_name})")
            return response_text

        except Exception as e:
            logger.error(
                f"âŒ AI Provider è°ƒç”¨å¤±è´¥ ({self.provider.__class__.__name__}): {e}"
            )
            return None

    def get_single_diff_summary(self, diff_content: str) -> Optional[str]:
        """
        (V3.4 é‡æ„) ä½¿ç”¨ AI å•ç‹¬æ€»ç»“ä¸€ä¸ª diff çš„æ ¸å¿ƒé€»è¾‘å˜æ›´ã€‚
        """
        if len(diff_content) > 100000:
            logger.warning(
                f"âš ï¸ Diff å†…å®¹è¿‡é•¿ ({len(diff_content)} chars)ï¼Œè·³è¿‡ AI æ€»ç»“ã€‚"
            )
            return "(Diff å†…å®¹è¿‡é•¿ï¼Œå·²è·³è¿‡æ€»ç»“)"

        summary = self._generate_content("diff_map.txt", {"diff_content": diff_content})

        # (V3.3) V3.3 çš„ç‰¹å®šåå¤„ç†
        if summary:
            return summary.strip().replace("\n", " ")
        return None

    def get_ai_summary(
        self,
        text_report: str,
        diff_summaries: Optional[str] = None,
        previous_summary: Optional[str] = None,
    ) -> Optional[str]:
        """
        (V3.4 é‡æ„) ä½¿ç”¨ AI ç”Ÿæˆæœ€ç»ˆçš„å·¥ä½œæ‘˜è¦ã€‚
        """
        # (V3.3) å‡†å¤‡ç”¨äºæ¨¡æ¿çš„åŠ¨æ€å†…å®¹å—
        history_block = (
            f"""
        --- è¿™æ˜¯ä½ æ˜¨å¤©çš„å·¥ä½œæ‘˜è¦ï¼ˆå†å²ä¸Šä¸‹æ–‡ï¼‰ ---
        {previous_summary}
        --- å†å²ä¸Šä¸‹æ–‡ç»“æŸ ---
        """
            if previous_summary and previous_summary.strip()
            else ""
        )

        diff_block = (
            f"""
        --- ä»Šå¤© AI ç”Ÿæˆçš„é€æ¡ä»£ç å˜æ›´æ€»ç»“ (Diffs) ---
        {diff_summaries}
        --- ä»£ç å˜æ›´æ€»ç»“ç»“æŸ ---
        """
            if diff_summaries and diff_summaries.strip()
            else ""
        )

        return self._generate_content(
            "summary_reduce.txt",
            {
                "history_block": history_block,
                "text_report": text_report,
                "diff_block": diff_block,
            },
        )

    def distill_project_memory(self) -> Optional[str]:
        """
        (V3.4 é‡æ„) (è®°å¿†è’¸é¦) è¯»å–å†å²æ—¥å¿—ï¼Œç”Ÿæˆæµ“ç¼©è®°å¿†ã€‚
        """
        logger.info("ğŸ§  æ­£åœ¨å¯åŠ¨ AI 'è®°å¿†è’¸é¦' é˜¶æ®µ...")

        log_file_path = os.path.join(
            self.config.PROJECT_DATA_PATH, self.config.PROJECT_LOG_FILE
        )

        try:
            with open(log_file_path, "r", encoding="utf-8") as f:
                full_log = f.read()
        except FileNotFoundError:
            logger.info(f"â„¹ï¸ æœªæ‰¾åˆ°é¡¹ç›®æ—¥å¿— ({log_file_path})ï¼Œå°†åˆ›å»ºæ–°è®°å¿†ã€‚")
            return None
        except Exception as e:
            logger.error(f"âŒ è¯»å–é¡¹ç›®æ—¥å¿—å¤±è´¥ ({log_file_path}): {e}")
            return None

        if not full_log.strip():
            logger.info("â„¹ï¸ é¡¹ç›®æ—¥å¿—ä¸ºç©ºï¼Œæ— éœ€è’¸é¦ã€‚")
            return None

        return self._generate_content("memory_distill.txt", {"full_log": full_log})

    def generate_public_article(
        self,
        today_technical_summary: str,
        project_historical_memory: str,
        project_readme: Optional[str] = None,
    ) -> Optional[str]:
        """
        (V3.4 é‡æ„) è½¬æ¢ä¸ºé¢å‘å…¬ä¼—çš„å…¬ä¼—å·æ–‡ç« ã€‚
        """
        logger.info("âœï¸ æ­£åœ¨å¯åŠ¨ AI 'é£æ ¼è½¬æ¢' é˜¶æ®µ (ç”Ÿæˆå…¬ä¼—å·æ–‡ç« )...")

        # (V3.3) å‡†å¤‡ç”¨äºæ¨¡æ¿çš„åŠ¨æ€å†…å®¹å—
        readme_block = (
            f"""
        3.  **é¡¹ç›® README (ä½¿å‘½ä¸æ„¿æ™¯)**:
            (è¿™èƒ½è®©ä½ ç†è§£é¡¹ç›®çš„æ ¸å¿ƒä»·å€¼å’Œç›®æ ‡ç”¨æˆ·)
            ---
            {project_readme}
            ---
        """
            if project_readme
            else ""
        )

        return self._generate_content(
            "public_article.txt",
            {
                "project_historical_memory": project_historical_memory,
                "today_technical_summary": today_technical_summary,
                "readme_block": readme_block,
            },
        )
