# ai_summarizer.py
import logging
import sys
from typing import Optional
from config import GitReportConfig
import os

try:
    import google.generativeai as genai
except ImportError:
    print("é”™è¯¯: google-generativeai åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install google-generativeai")
    sys.exit(1)

logger = logging.getLogger(__name__)


# (V2.4 é‡æ„: æ•´ä¸ªæ–‡ä»¶è¢«é‡æ„ä¸º AIService ç±»)


class AIService:
    """
    å°è£…æ‰€æœ‰å¯¹ Google Gemini AI çš„è°ƒç”¨ã€‚
    åœ¨åˆå§‹åŒ–æ—¶é…ç½®ä¸€æ¬¡æ¨¡å‹ï¼Œä¾›æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ã€‚
    """

    def __init__(self, config: GitReportConfig):
        """
        åˆå§‹åŒ– AI æœåŠ¡ï¼ŒåŠ è½½é…ç½®å¹¶é…ç½®ä¸€æ¬¡ GenAI æ¨¡å‹ã€‚
        """
        self.config = config
        # (V2.4 é‡æ„: åœ¨åˆå§‹åŒ–æ—¶è°ƒç”¨ä¸€æ¬¡ï¼Œå¹¶å­˜å‚¨æ¨¡å‹å®ä¾‹)
        self.model = self._configure_genai()
        if self.model:
            logger.info("ğŸ¤– AI æœåŠ¡å·²æˆåŠŸåˆå§‹åŒ– (Gemini 2.5 Flash)")
        else:
            logger.error("âŒ AI æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œåç»­ AI åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

    def _configure_genai(self) -> Optional[genai.GenerativeModel]:  # type: ignore
        """
        (V2.4 é‡æ„: è½¬æ¢ä¸ºç§æœ‰æ–¹æ³•)
        è¾…åŠ©å‡½æ•°ï¼Œç”¨äºé…ç½® GenAIï¼Œé¿å…ä»£ç é‡å¤ã€‚
        """
        # (V2.4 é‡æ„: ä½¿ç”¨ self.config)
        if not self.config.AI_API_KEY:
            logger.warning("âŒ æœªé…ç½® GOOGLE_API_KEY ç¯å¢ƒå˜é‡")
            return None
        try:
            # (V2.4 é‡æ„: ä½¿ç”¨ self.config)
            genai.configure(api_key=self.config.AI_API_KEY)  # type: ignore
            model = genai.GenerativeModel("gemini-2.5-flash")  # type: ignore
            return model
        except Exception as e:
            logger.error(f"âŒ GenAI é…ç½®å¤±è´¥: {e}")
            return None

    # --- (V3.3) æ–°å¢: Prompt åŠ è½½å™¨ ---
    def _load_prompt_template(self, template_name: str) -> Optional[str]:
        """(V3.3) è¾…åŠ©å‡½æ•°ï¼šä» prompts/ ç›®å½•åŠ è½½æ¨¡æ¿"""
        prompt_path = os.path.join(
            self.config.SCRIPT_BASE_PATH, "prompts", template_name
        )
        try:
            with open(prompt_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            logger.error(f"âŒ (V3.3) Prompt æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {prompt_path}")
            return None
        except Exception as e:
            logger.error(f"âŒ (V3.3) åŠ è½½ Prompt æ¨¡æ¿å¤±è´¥ ({prompt_path}): {e}")
            return None

    # --- (V3.3) ç»“æŸ ---

    def get_single_diff_summary(self, diff_content: str) -> Optional[str]:
        """
        (V2.4 é‡æ„: è½¬æ¢ä¸ºæ–¹æ³•)
        (V3.3 ä¿®æ”¹: ä» prompts/diff_map.txt åŠ è½½ Prompt)
        (æ–°å¢ "Map" é˜¶æ®µ)
        ä½¿ç”¨ AI å•ç‹¬æ€»ç»“ä¸€ä¸ª diff çš„æ ¸å¿ƒé€»è¾‘å˜æ›´ã€‚
        """
        if not self.model:
            return None

        # (V3.3) åŠ è½½ Prompt
        prompt_template = self._load_prompt_template("diff_map.txt")
        if not prompt_template:
            return None

        logger.info("ğŸ¤– æ­£åœ¨è°ƒç”¨ AI æ€»ç»“å•ä¸ª Diff...")

        if len(diff_content) > 100000:
            logger.warning(
                f"âš ï¸ Diff å†…å®¹è¿‡é•¿ ({len(diff_content)} chars)ï¼Œè·³è¿‡ AI æ€»ç»“ã€‚"
            )
            return "(Diff å†…å®¹è¿‡é•¿ï¼Œå·²è·³è¿‡æ€»ç»“)"

        # (V3.3) æ ¼å¼åŒ– Prompt
        prompt = prompt_template.format(diff_content=diff_content)

        try:
            response = self.model.generate_content(prompt)
            summary = response.text.strip().replace("\n", " ")
            logger.info(f"âœ… å•ä¸ª Diff æ€»ç»“æˆåŠŸ: {summary}")
            return summary
        except Exception as e:
            logger.error(f"âŒ å•ä¸ª Diff æ€»ç»“å¤±è´¥: {e}")
            return None

    def get_ai_summary(
        self,
        text_report: str,
        diff_summaries: Optional[str] = None,
        previous_summary: Optional[str] = None,
    ) -> Optional[str]:
        """
        (V3.3 ä¿®æ”¹: ä» prompts/summary_reduce.txt åŠ è½½ Prompt)
        ä½¿ç”¨ AI ç”Ÿæˆæœ€ç»ˆçš„å·¥ä½œæ‘˜è¦ã€‚
        """
        logger.info("ğŸ¤– æ­£åœ¨è°ƒç”¨ AI ç”Ÿæˆ*æœ€ç»ˆ*æ‘˜è¦...")

        if not self.model:
            return None

        # (V3.3) åŠ è½½ Prompt
        prompt_template = self._load_prompt_template("summary_reduce.txt")
        if not prompt_template:
            return None

        # (V3.3) å‡†å¤‡ç”¨äºæ¨¡æ¿çš„åŠ¨æ€å†…å®¹å—
        history_block = (
            f"""
        --- è¿™æ˜¯ä½ æ˜¨å¤©çš„å·¥ä½œæ‘˜è¦ï¼ˆå†å²ä¸Šä¸‹æ–‡ï¼‰ ---
        {previous_summary}
        --- å†å²ä¸Šä¸‹æ–‡ç»“æŸ ---
        """
            if previous_summary and previous_summary.strip()
            else ""
        )

        diff_block = (
            f"""
        --- ä»Šå¤© AI ç”Ÿæˆçš„é€æ¡ä»£ç å˜æ›´æ€»ç»“ (Diffs) ---
        {diff_summaries}
        --- ä»£ç å˜æ›´æ€»ç»“ç»“æŸ ---
        """
            if diff_summaries and diff_summaries.strip()
            else ""
        )

        # (V3.3) æ ¼å¼åŒ– Prompt
        prompt = prompt_template.format(
            history_block=history_block, text_report=text_report, diff_block=diff_block
        )

        try:
            response = self.model.generate_content(prompt)
            logger.info("âœ… AI æœ€ç»ˆæ‘˜è¦ç”ŸæˆæˆåŠŸ (å·²åŒ…å«å†å²ä¸Šä¸‹æ–‡)")
            return response.text

        except Exception as e:
            logger.error(f"âŒ AI æœ€ç»ˆæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def distill_project_memory(self) -> Optional[str]:
        """
        (V3.1 ä¿®æ”¹)
        (V3.3 ä¿®æ”¹: ä» prompts/memory_distill.txt åŠ è½½ Prompt)
        (è®°å¿†è’¸é¦) è¯»å– *æ‰€æœ‰* çš„å†å²æ—¥å¿—ï¼Œç”Ÿæˆä¸€ä¸ªæµ“ç¼©çš„ã€æœ‰æƒé‡çš„è®°å¿†æ–‡ä»¶ã€‚
        """
        logger.info("ğŸ§  æ­£åœ¨å¯åŠ¨ AI 'è®°å¿†è’¸é¦' é˜¶æ®µ...")

        log_file_path = os.path.join(
            self.config.PROJECT_DATA_PATH, self.config.PROJECT_LOG_FILE
        )

        try:
            with open(log_file_path, "r", encoding="utf-8") as f:
                full_log = f.read()
        except FileNotFoundError:
            logger.info(f"â„¹ï¸ æœªæ‰¾åˆ°é¡¹ç›®æ—¥å¿— ({log_file_path})ï¼Œå°†åˆ›å»ºæ–°è®°å¿†ã€‚")
            return None
        except Exception as e:
            logger.error(f"âŒ è¯»å–é¡¹ç›®æ—¥å¿—å¤±è´¥ ({log_file_path}): {e}")
            return None

        if not full_log.strip():
            logger.info("â„¹ï¸ é¡¹ç›®æ—¥å¿—ä¸ºç©ºï¼Œæ— éœ€è’¸é¦ã€‚")
            return None

        if not self.model:
            return None

        # (V3.3) åŠ è½½ Prompt
        prompt_template = self._load_prompt_template("memory_distill.txt")
        if not prompt_template:
            return None

        # (V3.3) æ ¼å¼åŒ– Prompt
        prompt = prompt_template.format(full_log=full_log)

        try:
            response = self.model.generate_content(prompt)
            logger.info("âœ… AI 'è®°å¿†è’¸é¦' æˆåŠŸ")
            return response.text
        except Exception as e:
            logger.error(f"âŒ AI 'è®°å¿†è’¸é¦' å¤±è´¥: {e}")
            return None

    def generate_public_article(
        self,
        today_technical_summary: str,
        project_historical_memory: str,
        project_readme: Optional[str] = None,
    ) -> Optional[str]:
        """
        (V3.3 ä¿®æ”¹: ä» prompts/public_article.txt åŠ è½½ Prompt)
        å°†æŠ€æœ¯æ‘˜è¦å’Œé¡¹ç›®å†å²ï¼Œè½¬æ¢ä¸ºé¢å‘å…¬ä¼—çš„å…¬ä¼—å·æ–‡ç« ï¼Œå¹¶åˆ©ç”¨ README æ–‡ä»¶ã€‚
        """
        logger.info("âœï¸ æ­£åœ¨å¯åŠ¨ AI 'é£æ ¼è½¬æ¢' é˜¶æ®µ (ç”Ÿæˆå…¬ä¼—å·æ–‡ç« )...")

        if not self.model:
            return None

        # (V3.3) åŠ è½½ Prompt
        prompt_template = self._load_prompt_template("public_article.txt")
        if not prompt_template:
            return None

        # (V3.3) å‡†å¤‡ç”¨äºæ¨¡æ¿çš„åŠ¨æ€å†…å®¹å—
        readme_block = (
            f"""
        3.  **é¡¹ç›® README (ä½¿å‘½ä¸æ„¿æ™¯)**:
            (è¿™èƒ½è®©ä½ ç†è§£é¡¹ç›®çš„æ ¸å¿ƒä»·å€¼å’Œç›®æ ‡ç”¨æˆ·)
            ---
            {project_readme}
            ---
        """
            if project_readme
            else ""
        )

        # (V3.3) æ ¼å¼åŒ– Prompt
        prompt = prompt_template.format(
            project_historical_memory=project_historical_memory,
            today_technical_summary=today_technical_summary,
            readme_block=readme_block,
        )

        try:
            response = self.model.generate_content(prompt)
            logger.info("âœ… AI 'é£æ ¼è½¬æ¢' æˆåŠŸ (å·²åŒ…å«é¡¹ç›®èƒŒæ™¯)")
            return response.text
        except Exception as e:
            logger.error(f"âŒ AI 'é£æ ¼è½¬æ¢' å¤±è´¥: {e}")
            return None
